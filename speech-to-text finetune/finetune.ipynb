{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e9db61-07ad-4efb-b415-01a0278a3f24",
   "metadata": {},
   "source": [
    "# Finetuning OpenAI's Whisper for Singaporean speech\n",
    "This fine tune of OpenAI's state of the art Automatic Speech Recognition (ASR) model aims to tackle a few naunces in the way Singaporeans speak that the original model struggles to recognise. More specifically the...\n",
    "1. Borrowing of words from other languages within the same sentence. Examples include the use of words like 'makan', 'kiasu', 'paiseh'.\n",
    "    - 'I understand that this math topic can be tough, but don't feel paiseh to ask questions if you're confused'.\n",
    "2. Use of particles such as 'lor', 'leh' and 'lah' that can alter the meaning or perceived tone of sentences.\n",
    "    - 'You can redo the assignment lor'. This sounds as if the teacher is allowing the redo of the assignment but not fully enthusiastic about the redo.\n",
    "    - 'You can redo the assignment lah'. This sounds like a straightforward suggestion or gentle encouragement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496cf11-0c24-4c65-a992-102bfaa2a572",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "For some context, the finetuned ASR model will play a crucial role in enhancing students' study efficiency through its transcription capabilities in our 'iOrganise' application. This model will handle transcriptions for both standalone audio files and audio within video files, making it easier for users to review audio-based study materials. By providing text transcripts, we aim to streamline the review process, as reading typically takes less time than listening to the original audio. Additionally, the transcribed text will feed into the subject classification pipeline, a feature that another group member of mine would be developing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee3b56d7-ea69-49f3-9a2b-c6d1c6aa1613",
   "metadata": {},
   "source": [
    "![appdiagram.png](diagrams/appdiagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3cf87-1d12-4b49-af79-43b6bf57e15a",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "I will begin with importing the necessary libraries. These libraries provides us with functions and tools that we will be using in order to finetune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c794e9-e745-4f00-bd29-df5b5544b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import chardet\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import IterableDatasetDict, load_dataset\n",
    "from pydub import AudioSegment\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, Seq2SeqTrainingArguments, Seq2SeqTrainer, GenerationConfig\n",
    "\n",
    "from local_loadingScript import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece097ee-cbdf-4bf9-9265-160d9d17e0bc",
   "metadata": {},
   "source": [
    "As I will be training our model using a GPU, I will check if the necessary libraries are correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98809076-a923-4222-b4bb-263dc93af249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec6774-cf8e-4513-bbb2-272597cb9c04",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "Collecting and preparing data for ASR can be challenging and tedious process. However, thankfully we have a useful dataset that we can use to finetune our model, the [National Speech Corpus (NSC)](https://www.imda.gov.sg/about-imda/emerging-technologies-and-research/artificial-intelligence/national-speech-corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704c445-fbc9-4818-a3ae-125d079ddf05",
   "metadata": {},
   "source": [
    "The corpus is shared via Dropbox and consists of 6 different parts , each part containing roughly 1000 hours of audio and their respective transcripts:\n",
    "1. Prompted recordings of phonetically-balanced scripts.\n",
    "2. Prompted recordings of sentences randomly generated from words based on people, food, location, brands, etc.\n",
    "3. General conversational data on topics covering daily life and playing games recorded in both seperate room and same room environments.\n",
    "4. Conversational data that includes code-switching such as from Singaporean English to other languages such as Mandarin, Malay, etc.\n",
    "5. Conversational data on debate, finance topics, with positive and negative emotion.\n",
    "6. Conversational data in 3 styles (holiday/restaurant/hotel, bank/telephone/insurance, HDB/MOE/MSF).\n",
    "\n",
    "Our application is designed for O, N, and A level students in Singapore, with expected audio files primarily coming from classroom settings or video lectures. The speech in these recordings may vary between structured and conversational formats. To address challenges the pre-trained Whisper model encounters, I decided to focus on Part 3 of the NSC for finetuning our model, due to its conversational nature which should be able to better represent the kind of speech typically found in classrooms. \n",
    "\n",
    "The NSC Part 3 recordings are divided into two environments, each recorded with two different types of microphones. In the first environment, where speakers were in the same room, we selected recordings captured with the close-talk microphone to better isolate the main speaker’s voice. In the second environment, where speakers were in separate rooms, we chose recordings from the standing microphone instead of the telephone recordings.\n",
    "\n",
    "I downloaded the 4 folders and organised them in the following format:\n",
    "```\n",
    "dataset\n",
    "│\n",
    "├── audio\n",
    "│   ├── 3000-1.wav                              # Audio Same CloseMic\n",
    "│   ├── 3000-2.wav                              # Audio Same CloseMic\n",
    "│   ├── conf_2500_2500_00862025.wav             # Audio Seperate StandingMic\n",
    "│   └── conf_2500_2500_00862177.wav             # Audio Seperate StandingMic\n",
    "│\n",
    "└── transcripts\n",
    "    ├── 3000-1.TextGrid                         # Scripts Same\n",
    "    ├── 3000-2.TextGrid                         # Scripts Same\n",
    "    ├── conf_2500_2500_00862025.TextGrid        # Scripts Seperate\n",
    "    └── conf_2500_2500_00862177.TextGrid        # Scripts Seperate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e077186-fdbf-449a-9367-71aa63b0f42b",
   "metadata": {},
   "source": [
    "## Raw Data\n",
    "Taking a look at our raw data, we should be expecting 1 transcript to each audio file. However, that is not the case as we have 492 audio files in the 'Audio Same CloseMic' folder but 494 transcripts in the 'Scripts Same' folder. Similarly, we have 431 audio files in the 'Audio Seperate StandingMic' folder and only 418 transcripts in the 'Scripts Seperate' folder. As such, we need to find the outliers and remove the appropriate files so we have the correct amount of audio files to transcript files before we can proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb7d15b-ff6f-48cc-88cb-235a88358e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining files in the audio directory: 902\n",
      "Remaining files in the transcript directory: 902\n"
     ]
    }
   ],
   "source": [
    "def remove_unmatched_files(audio_dir, transcript_dir):\n",
    "    audio_files = os.listdir(audio_dir)\n",
    "    transcript_files = os.listdir(transcript_dir)\n",
    "\n",
    "    # remove the file extensions for comparison\n",
    "    audio_files_base = [os.path.splitext(f)[0] for f in audio_files]\n",
    "    transcript_files_base = [os.path.splitext(f)[0] for f in transcript_files]\n",
    "\n",
    "    unmatched_audio_files = []\n",
    "    unmatched_transcript_files = []\n",
    "\n",
    "    # check for every audio file if there's a corresponding transcript\n",
    "    for audio_base in audio_files_base:\n",
    "        if audio_base not in transcript_files_base:\n",
    "            unmatched_audio_files.append(audio_base + '.wav')\n",
    "\n",
    "    # check for every transcript file if there's a corresponding audio\n",
    "    for transcript_base in transcript_files_base:\n",
    "        if transcript_base not in audio_files_base:\n",
    "            unmatched_transcript_files.append(transcript_base + '.TextGrid')\n",
    "\n",
    "    if unmatched_audio_files:\n",
    "        for file in unmatched_audio_files:\n",
    "            # Delete unmatched audio files\n",
    "            os.remove(os.path.join(audio_dir, file))\n",
    "\n",
    "    if unmatched_transcript_files:\n",
    "        for file in unmatched_transcript_files:\n",
    "            # delete unmatched transcript files\n",
    "            os.remove(os.path.join(transcript_dir, file))\n",
    "\n",
    "    return unmatched_audio_files, unmatched_transcript_files\n",
    "\n",
    "# remove unmatched files\n",
    "unmatched_audio, unmatched_transcripts = remove_unmatched_files('dataset/audio', 'dataset/transcripts')\n",
    "\n",
    "print('\\nRemaining files in the audio directory:', len(os.listdir('dataset/audio')))\n",
    "print('Remaining files in the transcript directory:', len(os.listdir('dataset/transcripts')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aff8fe-8021-4f34-b2c2-3181003f5948",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "Whisper works by converting features within a 30s chunk of audio, represented as a log-Mel spectogram, into text. In order for us to train the model, we need to first process our raw dataset. We will do this by creating functions that:\n",
    "1. Clean transcripts by removing annotations for paralinguistic phenomena, fillers, unknown words, unclear words, short pauses, and removing annotations for particles to make it part of the text.\n",
    "2. Normalise transcripts by removing punctuation and converting text to lowercase.\n",
    "4. Splice out utterances from the audio using timestamps provided in transcripts and combine shorter consecutive segments such that each segment is no longer than 30s at a sampling rate of 16kHz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e67899-cd7c-4023-a2bb-ca9b61052d2c",
   "metadata": {},
   "source": [
    "I will start by defining a 'clean_transcript' function that will do the following:\n",
    "- Remove annotations for breathing, coughing and laughing represented in the transcript by annotations such as (ppo), (ppb), (ppl), etc.\n",
    "- Remove fillers or unknown words '\\<FIL/>', unclear words '\\<UNK>', short pauses '\\<S>' etc.\n",
    "- Remove annotations for particles like '[lah]' and keep it as part of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed610023-84eb-4c64-95ab-14d5beb329c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transcript(lines):\n",
    "    # pattern to match text inside quotes after 'text ='\n",
    "    pattern = r'(?<=text = \")(.*?)(?=\")'\n",
    "\n",
    "    cleaned_lines = []\n",
    "        \n",
    "    for line in lines:\n",
    "        # find and clean the 'text' field (inside the quotes after 'text = ')\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            original_text = match.group(0)  # get the 'text' content\n",
    "\n",
    "            # handle edge case where parentheses/angled brackets may appear at the end of the string or before a punctuation \n",
    "            if bool(re.search(r'(\\(.*?\\)|\\<.*?\\>)(?=\\s*[\\.,!?;]|\\s*$)', original_text)):\n",
    "                # remove parentheses and their contents along with the space before them\n",
    "                # remove angled brackets and their contents along with the space before them\n",
    "                # remove square brackets but keep contents\n",
    "                cleaned_text = re.sub(r'\\s?(\\(.*?\\)|\\<.*?\\>)|\\[|\\]', '', original_text)\n",
    "            else:\n",
    "                # remove parentheses and their contents along with the space after them\n",
    "                # remove angled brackets and their contents along with the space after them\n",
    "                # remove square brackets but keep contents\n",
    "                cleaned_text = re.sub(r'\\(.*?\\)\\s?|\\<.*?\\>\\s?|\\[|\\]', '', original_text)\n",
    "\n",
    "            # replace the original text with the cleaned version\n",
    "            cleaned_line = line.replace(original_text, cleaned_text)\n",
    "            \n",
    "        else:\n",
    "            cleaned_line = line\n",
    "\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "\n",
    "    return cleaned_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755318c-2fe7-4e8d-a9f4-fe87222e28aa",
   "metadata": {},
   "source": [
    "Moving on, I will define another function to help normalise the transcript by removing punctuation and converting the text to lower case to avoid inconsistent casings and punctuation after finetuning our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c8db0e-45c7-4e0f-aea5-3713973bdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_transcript(lines):\n",
    "    # pattern to match text inside quotes after 'text ='\n",
    "    pattern = r'(?<=text = \")(.*?)(?=\")'\n",
    "    \n",
    "    normalised_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            original_text = match.group(0)\n",
    "            \n",
    "            # remove unwanted punctuation, if needed, and convert to lowercase\n",
    "            normalised_text = re.sub(r'[^\\w\\s]', '', original_text).lower()\n",
    "\n",
    "            # Replace the original text with the normalized version\n",
    "            normalised_line = line.replace(original_text, normalised_text)\n",
    "\n",
    "        else:\n",
    "            normalised_line = line\n",
    "\n",
    "        # Add the normalized line to the list\n",
    "        normalised_lines.append(normalised_line)\n",
    "    \n",
    "    return normalised_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4660a-795a-43e1-88b0-660dd82e8a38",
   "metadata": {},
   "source": [
    "Here, I will define a function that takes in the path to a directory to process all the TextGrid transcripts inside it using the previously defined data pre-processing functions. All the processed transcripts would be stored in the 'transcripts_processed' folder, shown in the diagram below:\n",
    "```\n",
    "dataset\n",
    "│\n",
    "├── audio\n",
    "│   ├── 3000-1.wav                              # from 'NSC Part 3/Audio Same CloseMic'\n",
    "│   ├── 3000-2.wav                              # from 'NSC Part 3/Audio Same CloseMic'\n",
    "│   └── conf_2500_2500_00862025.wav             # from 'NSC Part 3/Audio Seperate StandingMic'\n",
    "│\n",
    "├── transcripts\n",
    "│   ├── 3000-1.TextGrid                         # from 'NSC Part 3/Scripts Same'\n",
    "│   ├── 3000-2.TextGrid                         # from 'NSC Part 3/Scripts Same'\n",
    "│   └── conf_2500_2500_00862025.TextGrid        # from 'NSC Part 3/Scripts Seperate'\n",
    "│\n",
    "└── transcripts_processed                       # (Newly created folder)\n",
    "    ├── 3000-1_processed.TextGrid               # (processed transcript)\n",
    "    └── 3000-2_processed.TextGrid               # (processed transcript)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25be2e51-959f-4829-a692-737eca58e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def detect_encoding(file_path):\n",
    "    return await asyncio.to_thread(read_file_detect_encoding, file_path)\n",
    "\n",
    "def read_file_detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw_data = file.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "    return encoding\n",
    "\n",
    "# helper function to read file content asynchronously\n",
    "async def read_file(file_path, encoding):\n",
    "    return await asyncio.to_thread(read_file_content, file_path, encoding)\n",
    "\n",
    "def read_file_content(file_path, encoding):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# helper function to write file content asynchronously\n",
    "async def write_file(file_path, data):\n",
    "    await asyncio.to_thread(write_file_content, file_path, data)\n",
    "\n",
    "def write_file_content(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(data)\n",
    "\n",
    "async def process_transcript(lines):\n",
    "    return await asyncio.to_thread(normalise_and_clean, lines)\n",
    "\n",
    "def normalise_and_clean(lines):\n",
    "    cleaned = clean_transcript(lines)\n",
    "    return normalise_transcript(cleaned)\n",
    "\n",
    "async def process_transcripts(input_folder, output_folder='dataset/transcripts_processed'):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Error: The folder {input_folder} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    tasks = []  # List to hold all the tasks for concurrency\n",
    "\n",
    "    # iterate over all files in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "        if os.path.isfile(file_path):\n",
    "            # detect encoding asynchronously\n",
    "            encoding = await detect_encoding(file_path)\n",
    "\n",
    "            # read the file asynchronously\n",
    "            lines = await read_file(file_path, encoding)\n",
    "\n",
    "            # process the transcript (normalizing and cleaning) concurrently\n",
    "            processed_transcript = await process_transcript(lines)\n",
    "\n",
    "            # get the processed file path\n",
    "            processed_path = os.path.join(output_folder, os.path.basename(file_path).replace(\".TextGrid\", \"_processed.TextGrid\"))\n",
    "\n",
    "            # write the processed transcript to a file asynchronously\n",
    "            tasks.append(write_file(processed_path, processed_transcript))\n",
    "\n",
    "    # wait for all tasks to complete asynchronously\n",
    "    await asyncio.gather(*tasks)\n",
    "    print(f\"All files processed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa8dd1-db2b-41d4-aebb-bc4d0521ce94",
   "metadata": {},
   "source": [
    "Let us begin with the file pre-processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6062cd5c-c1c4-485c-b176-561b5a0b43ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed and saved.\n"
     ]
    }
   ],
   "source": [
    "await process_transcripts('dataset/transcripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e360b3-5285-4d08-9422-7ae384f59a16",
   "metadata": {},
   "source": [
    "The transcripts that are stored in TextGrid format are structured in such a way where for each utterance, there is a timestamp associated with it. I will be defining a function that uses the timestamps to calculate the duration of all the utterances, extract said utterances from the audio and combine consecutive utterances from the audio to fit inside 1 sub 30s audio file. Similar to the audio files, the text portion in the transcript for consecutive utterances will also be combined and saved in a .txt file. The result of this step would be compact sub 30s audio files along with exactly what was said in them stored in the 'audio_segment_output_folder' and 'transcript_segment_output_folder'. For some context this is how the TextGrid file is formatted:\n",
    "```\n",
    "File type = \"ooTextFile\"\n",
    "Object class = \"TextGrid\"\n",
    "\n",
    "xmin = 0 \n",
    "xmax = 30.0\n",
    "tiers? <exists> \n",
    "size = 2\n",
    "item []:\n",
    "    item [1]:\n",
    "        class = \"IntervalTier\"\n",
    "        name = \"audio-1\"\n",
    "        xmin = 0 \n",
    "        xmax = 30.0\n",
    "        intervals: size = 3\n",
    "        intervals [1]:\n",
    "            xmin = 0 \n",
    "            xmax = 5.0 \n",
    "            text = \"Hello\"\n",
    "        intervals [2]:\n",
    "            xmin = 5.0 \n",
    "            xmax = 10.0 \n",
    "            text = \"how are you?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650b6141-6d62-4a5d-98cc-94538dd8b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splice_audio(\n",
    "        audio_path, \n",
    "        textgrid_path, \n",
    "        audio_segment_output_folder, \n",
    "        transcript_segment_output_folder, \n",
    "        max_duration=30\n",
    "    ):       \n",
    "    try:\n",
    "        original_audio = AudioSegment.from_file(audio_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # patterns for extracting intervals, xmin, xmax, and text\n",
    "    interval_pattern = re.compile(r'intervals \\[\\d+\\]:\\s*(.*?)\\s*(?=intervals|\\Z)', re.DOTALL)\n",
    "    xmin_pattern = re.compile(r'xmin = ([\\d.]+)')\n",
    "    xmax_pattern = re.compile(r'xmax = ([\\d.]+)')\n",
    "    text_pattern = re.compile(r'text = \"(.*?)\"')\n",
    "\n",
    "    try:\n",
    "        with open(textgrid_path, 'r', encoding='utf-8') as file:\n",
    "            textgrid_content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading TextGrid file {textgrid_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # find all interval blocks in the TextGrid file\n",
    "    intervals = interval_pattern.findall(textgrid_content)\n",
    "\n",
    "    # initialize lists for audio and transcript segments\n",
    "    combined_audio_segments = []\n",
    "    combined_transcript_segments = []\n",
    "\n",
    "    current_audio_segment = AudioSegment.empty()\n",
    "    current_transcript_segment = ''\n",
    "    total_duration = 0\n",
    "\n",
    "    for interval in intervals:\n",
    "        try:\n",
    "            # Extract xmin, xmax, and text values\n",
    "            xmin = float(xmin_pattern.search(interval).group(1))\n",
    "            xmax = float(xmax_pattern.search(interval).group(1))\n",
    "            text_match = text_pattern.search(interval)\n",
    "            text = text_match.group(1) if text_match else \"\"\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error extracting interval data from TextGrid: {e}\")\n",
    "            continue\n",
    "\n",
    "        # exclude utterances that are empty\n",
    "        if text == \"\":\n",
    "            continue\n",
    "\n",
    "        # calculate the duration of the interval\n",
    "        interval_duration = xmax - xmin\n",
    "\n",
    "        try:\n",
    "            # check if adding the current interval exceeds max duration\n",
    "            if total_duration + interval_duration > max_duration:\n",
    "                # if the current segment has data, save it\n",
    "                if current_audio_segment and current_transcript_segment != '':\n",
    "                    combined_audio_segments.append(current_audio_segment)\n",
    "                    combined_transcript_segments.append(current_transcript_segment.rstrip())\n",
    "\n",
    "                # start a new segment\n",
    "                current_audio_segment = original_audio[xmin * 1000 : xmax * 1000]\n",
    "                current_transcript_segment = text + ' '\n",
    "                total_duration = interval_duration\n",
    "            else:\n",
    "                # add current interval to the segment\n",
    "                current_audio_segment += original_audio[xmin * 1000 : xmax * 1000]\n",
    "                current_transcript_segment += text + ' '\n",
    "                total_duration += interval_duration\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing interval at {xmin}-{xmax} seconds: {e}\")\n",
    "            continue\n",
    "\n",
    "    # add the last segment if necessary\n",
    "    if current_audio_segment and current_transcript_segment != '':\n",
    "        combined_audio_segments.append(current_audio_segment)\n",
    "        combined_transcript_segments.append(current_transcript_segment.rstrip())\n",
    "\n",
    "    # export audio and transcript segments\n",
    "    try:\n",
    "        for id_, (audio_segment, transcript_segment) in enumerate(zip(combined_audio_segments, combined_transcript_segments)):\n",
    "            # save the audio segment as a .wav file\n",
    "            audio_segment.export(os.path.join(audio_segment_output_folder, f\"{os.path.splitext(os.path.basename(audio_path))[0]}_segment_{id_ + 1}.wav\"), format=\"wav\")\n",
    "        \n",
    "            # save the transcript segment as a .txt file\n",
    "            with open(os.path.join(transcript_segment_output_folder, f\"{os.path.splitext(os.path.basename(textgrid_path))[0]}_segment_{id_ + 1}.txt\"), 'w', encoding='utf-8') as transcript_file:\n",
    "                transcript_file.write(transcript_segment)\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting audio or transcript segments: {e}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93663d-a643-4ad1-a0eb-d4dec9f33a01",
   "metadata": {},
   "source": [
    "I will make use of the function defined to splice all audio files inside my 'audio' and 'transcripts_processed' folder with the transcript and audio segments going into 'the 'transcript_segments' and 'audio_segments' folders. Multithredding is used due to the large number of files that needs to be spliced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2352bc-7f43-4b79-8088-f8f86eb23c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All audio files and their corresponding transcript have been spliced and saved.\n"
     ]
    }
   ],
   "source": [
    "def process_audio_files_in_directory(audio_dir, transcript_dir, audio_segment_output_folder, transcript_segment_output_folder):\n",
    "    # ThreadPoolExecutor to process files concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for audio_filename in os.listdir(audio_dir):\n",
    "            if audio_filename.endswith('.wav'):\n",
    "                audio_path = os.path.join(audio_dir, audio_filename)\n",
    "                textgrid_filename = audio_filename.replace('.wav', '_processed.TextGrid')\n",
    "                textgrid_path = os.path.join(transcript_dir, textgrid_filename)\n",
    "\n",
    "                if os.path.exists(textgrid_path):\n",
    "                    # submit the task to the executor for processing\n",
    "                    futures.append(executor.submit(splice_audio, audio_path, textgrid_path, audio_segment_output_folder, transcript_segment_output_folder))\n",
    "                else:\n",
    "                    print(f\"Warning: No corresponding TextGrid file for {audio_filename}. Skipping.\")\n",
    "        \n",
    "        # wait for all futures to complete\n",
    "        for future in futures:\n",
    "            future.result()  # this will raise exceptions if there were any during execution\n",
    "\n",
    "    print(f\"All audio files and their corresponding transcript have been spliced and saved.\")\n",
    "\n",
    "\n",
    "audio_dir = 'dataset/audio'\n",
    "transcript_dir = 'dataset/transcripts_processed'\n",
    "audio_segment_output_folder='dataset/audio_segments' \n",
    "transcript_segment_output_folder='dataset/transcript_segments'\n",
    "\n",
    "# create the output folders if it doesn't exist\n",
    "if not os.path.exists(audio_segment_output_folder):\n",
    "    os.makedirs(audio_segment_output_folder)\n",
    "\n",
    "if not os.path.exists(transcript_segment_output_folder):\n",
    "    os.makedirs(transcript_segment_output_folder)\n",
    "    \n",
    "\n",
    "process_audio_files_in_directory(audio_dir, transcript_dir, audio_segment_output_folder, transcript_segment_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820aafb9-0a8c-41ad-8e67-4a333e582e95",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "The whole of NSC parts 3 and 4 consists of roughly 1000 hours of speech each which is a large amount of data that I will need to load into memory. In order to reduce memory usage, I made use of the IterableDatasetDict and implemented a data loading script which helps lazily load my dataset as needed. \n",
    "\n",
    "Whisper works by converting features within a 30s chunk of audio, represented as a log-Mel spectogram, into text. In order for us to train the model, we also need to convert each audio chunk into a log-Mel spectogram and tokenise the corresponding transcript. Here, I defined a function to do just that to process the data in the iterable dataset before using it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a215949-7530-4b9a-a1b1-dea7a2e895fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\datasets\\load.py:929: FutureWarning: The repository for local_loadingScript contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at local_loadingScript.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained('openai/whisper-small', task='transcribe')\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio_path = batch['audio_path']\n",
    "    transcript_path = batch['transcript_path']\n",
    "\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    audio = audio.set_frame_rate(16000)\n",
    "    audio_array = np.array(audio.get_array_of_samples())\n",
    "\n",
    "    with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "        transcript = f.read().strip()\n",
    "    \n",
    "    # perform feature extraction by computing log-Mel input features from input audio\n",
    "    batch['input_features'] = torch.tensor(processor.feature_extractor(audio_array, sampling_rate=16000).input_features[0])\n",
    "    # tokenize the transcripts\n",
    "    batch['labels'] = processor.tokenizer(transcript).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset_train = load_dataset('local_loadingScript.py','all', split='train', streaming=True)\n",
    "dataset_val = load_dataset('local_loadingScript.py','all', split='validation', streaming=True)\n",
    "\n",
    "dataset = IterableDatasetDict()\n",
    "dataset['train'] = dataset_train\n",
    "dataset['validation'] = dataset_val\n",
    "\n",
    "dataset_processed = dataset.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7340a3-9939-4856-b3e7-526f24e98c9d",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "Before we begin with the process of finetuning our model, let's first define the metrics we would be using for comparison purposes. One of the most commonly adopted metric for ASR tasks would be Word Error Rate (WER), which can be calculated as follows:\n",
    "\n",
    "$WER = \\frac{N}{S + D + I}$\n",
    "\n",
    "Where:\n",
    "- **S** is the number of substitutions\n",
    "- **D** is the number of deletions\n",
    "- **I** is the number of insertions\n",
    "- **N** is the number of words in the reference\n",
    "\n",
    "In this case, instead of writing my own function to calculate the WER, I will simply use the evaluate library to do it for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64551df9-535e-4a54-b877-c47d70cb3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate WER using predicted token array and ground truth token array\n",
    "metric = evaluate.load('wer')\n",
    "\n",
    "def compute_metrics(pred) -> Dict[str, float]:\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5425c0-f845-4423-9cf1-cd08806fe1b6",
   "metadata": {},
   "source": [
    "## Loading Model\n",
    "As this model is being trained locally using an RTX 4070 Ti Super, I will be making use of a technique called gradient checkpointing, which reduces memory usage but increases computation time to manage system resource requirements. However, the model originally comes with key Value (KV) caching enabled which poses a problem. While KV caching is used to help speed up transformer decoding during inference, it is incompatible with gradient checkpointing. As such, I will modify the model's configuration to disable KV caching along with setting up other model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8012ba59-daee-4d77-b951-23c2e2d9932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained model\n",
    "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n",
    "\n",
    "# set up generation config\n",
    "generation_config = GenerationConfig.from_pretrained('openai/whisper-small')\n",
    "generation_config.task = 'transcribe'\n",
    "generation_config.language = 'en'\n",
    "\n",
    "# disable KV caching on model\n",
    "model.config.use_cache = False\n",
    "# modify the behaviour of the original 'generate' function to enable KV caching\n",
    "model.generate = partial(model.generate, generation_config=generation_config, use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb64798-3667-4196-bce7-f0de166d7346",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "As we convert our 30s audio chunks and corresponding transcripts into audio features and tokenized labels, we need to ensure that they are of the same lengths. This custom data collator pads audio features and tokenized labels to the appropriate max length.adding tokens are then replaced with -100, so that they are subsequently ignored in the loss calculations during finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1e01284-44a8-4434-96b3-a4b9258ac6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{'input_features': feature['input_features']} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors='pt')\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{'input_ids': feature['labels']} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors='pt')\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch['input_ids'].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's appended later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch['labels'] = labels\n",
    "        batch['attention_mask'] = labels_batch['attention_mask']\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor, decoder_start_token_id=model.config.decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2db5c6-92e7-43c2-8931-8288ee210f94",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "I will be making use of Seq2SeqTrainer from Hugging Face's Transformers library  to finetune the Whisper model on my dataset. In total, we will be training our model on roughly 750 hours of audio from NSC Part 3 for 10000 steps at a learning rate of 40x smaller than what was used for pre-training using a training batch size of 32 with the rest of the audio going to the validation and test set (~95 hours each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c25747d-7eff-456f-a510-aa53d607fae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5251' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5251/8000 26:40:24 < 13:58:09, 0.05 it/s, Epoch 1.29/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.622784</td>\n",
       "      <td>24.665307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>0.521898</td>\n",
       "      <td>21.434722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.476975</td>\n",
       "      <td>17.635924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.469981</td>\n",
       "      <td>18.228255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.416100</td>\n",
       "      <td>0.476523</td>\n",
       "      <td>18.135082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>0.450703</td>\n",
       "      <td>17.288290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>0.438338</td>\n",
       "      <td>16.878659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.394200</td>\n",
       "      <td>0.427253</td>\n",
       "      <td>16.195979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.517900</td>\n",
       "      <td>0.417884</td>\n",
       "      <td>15.925871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.380800</td>\n",
       "      <td>0.403043</td>\n",
       "      <td>15.115784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.393427</td>\n",
       "      <td>14.724504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.386770</td>\n",
       "      <td>14.257934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.391813</td>\n",
       "      <td>14.968142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.395648</td>\n",
       "      <td>14.782502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.223500</td>\n",
       "      <td>0.396562</td>\n",
       "      <td>15.001788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.398505</td>\n",
       "      <td>14.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.394337</td>\n",
       "      <td>14.671448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.448400</td>\n",
       "      <td>0.395333</td>\n",
       "      <td>14.698505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.269400</td>\n",
       "      <td>0.392072</td>\n",
       "      <td>14.566981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.388185</td>\n",
       "      <td>14.635096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "C:\\Users\\Sean\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 36\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./whisper-finetune\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     logging_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     27\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     28\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mfeature_extractor,\n\u001b[0;32m     34\u001b[0m )\n\u001b[1;32m---> 36\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# save the tokeniser which includes the list of vocabulary\u001b[39;00m\n\u001b[0;32m     38\u001b[0m processor\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhisper-finetune/processor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\trainer.py:2467\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\trainer.py:2915\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2913\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2915\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m   2918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\trainer.py:2872\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2872\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2873\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3869\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\trainer.py:4061\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4058\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4060\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4061\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4062\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4063\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\trainer_seq2seq.py:310\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    306\u001b[0m ):\n\u001b[0;32m    307\u001b[0m     generation_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    308\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    309\u001b[0m     }\n\u001b[1;32m--> 310\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:671\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[1;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m             proc\u001b[38;5;241m.\u001b[39mset_begin_index(decoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[0;32m    665\u001b[0m (\n\u001b[0;32m    666\u001b[0m     seek_sequences,\n\u001b[0;32m    667\u001b[0m     seek_outputs,\n\u001b[0;32m    668\u001b[0m     should_skip,\n\u001b[0;32m    669\u001b[0m     do_condition_on_prev_tokens,\n\u001b[0;32m    670\u001b[0m     model_output_type,\n\u001b[1;32m--> 671\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:834\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate_with_fallback\u001b[1;34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    830\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    831\u001b[0m             generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, batch_size \u001b[38;5;241m-\u001b[39m cur_bsz), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    832\u001b[0m         )\n\u001b[1;32m--> 834\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    835\u001b[0m     segment_input,\n\u001b[0;32m    836\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    837\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m    838\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m    839\u001b[0m     prefix_allowed_tokens_fn\u001b[38;5;241m=\u001b[39mprefix_allowed_tokens_fn,\n\u001b[0;32m    840\u001b[0m     synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m    841\u001b[0m     decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m    842\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs,\n\u001b[0;32m    844\u001b[0m )\n\u001b[0;32m    846\u001b[0m model_output_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\generation\\utils.py:1864\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1864\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[0;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\generation\\utils.py:512\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[0;32m    510\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    511\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 512\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1121\u001b[0m, in \u001b[0;36mWhisperEncoder.forward\u001b[1;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1114\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1115\u001b[0m             hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1118\u001b[0m             output_attentions,\n\u001b[0;32m   1119\u001b[0m         )\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1121\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1128\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:672\u001b[0m, in \u001b[0;36mWhisperEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    670\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    671\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m--> 672\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    679\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:620\u001b[0m, in \u001b[0;36mWhisperSdpaAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# partitioned across GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[0;32m    618\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m--> 620\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./whisper-finetune',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5, #40 times smaller than what was used for pre-training\n",
    "    warmup_steps=500,\n",
    "    max_steps=8000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy='steps',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=250,\n",
    "    eval_steps=250,\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='wer',\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy='steps',\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset_processed['train'],\n",
    "    eval_dataset=dataset_processed['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# save the tokeniser which includes the list of vocabulary\n",
    "processor.save_pretrained('whisper-finetune/processor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f9eed-1995-4896-a00b-c22ec6956cbd",
   "metadata": {},
   "source": [
    "As I saw that the model's validation loss and WER stops improving, I decided to stop the training early as it is a sign that the model has converged. We can visualise the training process using tensorboard. Make sure to clear the temporary files under '.tensorboard-info' if it refuses to connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88659aa-aa2e-42da-b4f0-fa72fb6ed8c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f586ba9fd87dc946\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f586ba9fd87dc946\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229e21e-f95a-4721-bb24-c51b56311957",
   "metadata": {},
   "source": [
    "Now that I am done with finetuning the model, I will login to HuggingFace via the CLI before saving the model to my HuggingFace repository. In this case, I will be taking checkpoint 3000 because we can see that the validation loss and WER stops improving after that point and it is at it's lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb83c7ee-a80b-4737-93a5-b97fd11ab7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246228fe38fa4132a066550637212390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/455 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sean\\.cache\\huggingface\\hub\\models--Xycone--whisper-small-SGspeech-finetune. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a3e44503a945f8b002e3bdb8f3ee8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/455 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Xycone/whisper-small-SGspeech-finetune/commit/351b8f244d88e845687ef351d0311128a6e23c23', commit_message='Upload processor', commit_description='', oid='351b8f244d88e845687ef351d0311128a6e23c23', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_name = 'Xycone/whisper-small-SGspeech-finetune'\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained('whisper-finetune/checkpoint-3000')\n",
    "processor = WhisperProcessor.from_pretrained('whisper-finetune/processor')\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "processor.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f679ae9-0e02-4904-b155-0c1c38c5e0ea",
   "metadata": {},
   "source": [
    "## Model Evaluation: Pre-trained Vs Fine-tuned\n",
    "Now that I have trained the Whisper model, I will be comparing the trained model against the pre-trained model with some random examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "488461a2-456c-48be-89eb-9ba81e60fbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\anaconda3\\envs\\IT3100-AAP\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth:\n",
      "okay ya what is the oh okay self disappointed that means if they find out something about me then they will be disappointed that perhaps im that im selfish that i want to have my own space and not having to share with a in laws ya then like feeling that the the in laws staying with us is a intrusion to my family life yup ya ya is normal lor but is yes ya lor correct ya lor\n",
      "\n",
      "Pretrained Output:\n",
      " Okay, yeah, what is the oh, okay self disappointed. That means if they find out something about me Then they'll be disappointed that perhaps I'm that I'm selfish that I want to have my own space and not having to share with a in-laws yeah, then like Feeling that the the in-laws thing with us is intrusion to my family life. Yeah, yeah, it's normal\n",
      "\n",
      "Finetuned Output:\n",
      " okay yup what is the oh okay self disappointed that means if they find out something about me then theyll be disappointed that perhaps im that im selfish that i want to have my own space and not having to share with a in laws ya then like feeling that the the in laws thing with us is intrusion to my family life ya ya ya its normal lor but its yes ya lor correct ya lor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "transcript_path='dataset/transcript_segments/conf_2726_2726_00862618_processed_segment_50.txt'\n",
    "audio_path='dataset/audio_segments/conf_2726_2726_00862618_segment_50.wav'\n",
    "\n",
    "with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "    transcript = f.read().strip()\n",
    "\n",
    "pretrained_whisper = pipeline('automatic-speech-recognition', 'openai/whisper-small', return_timestamps=True, torch_dtype=torch.float16, device='cuda:0', generate_kwargs = {\"task\":\"transcribe\", \"language\":\"<|en|>\"})\n",
    "pretrained_output = pretrained_whisper(audio_path)\n",
    "\n",
    "finetuned_whisper = pipeline('automatic-speech-recognition', 'Xycone/whisper-small-SGspeech-finetune', return_timestamps=True, torch_dtype=torch.float16, device='cuda:0', generate_kwargs = {\"task\":\"transcribe\", \"language\":\"<|en|>\"})\n",
    "finetuned_output = finetuned_whisper(audio_path)\n",
    "\n",
    "print(f\"Ground Truth:\\n{transcript}\\n\")\n",
    "print(f\"Pretrained Output:\\n{pretrained_output['text']}\\n\")\n",
    "print(f\"Finetuned Output:\\n{finetuned_output['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a33afb-1e46-4014-bdfe-137564d6de1a",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "This notebook follows the methods outlined in the Medium post [\"Finetuning Whisper for the Singaporean Home Team Context\"](https://medium.com/htx-dsai/finetuning-whisper-for-the-singaporean-home-team-context-a3ae1a6ae809) by Rachel LW Tan and a blog post titled [\"Singlish-Whisper: Finetuning ASR for Singapore's Unique English\"](https://www.jensenlwt.com/blog/singlish-whisper-finetuning-asr-for-singapore-unique-english) by Jenson Low. Both author’s approach to finetuning and training OpenAI's Whisper model was applied in this notebook with some minor modifications tailored to my use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
